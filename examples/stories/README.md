# Hierarchical Neural Story Generation (Fan et al., 2018)

The following commands provide an example of pre-processing data, training a model, and generating text for story generation with the WritingPrompts dataset.

## Pre-trained models

Description | Dataset | Model | Test set(s)
---|---|---|---
Stories with Convolutional Model <br> ([Fan et al., 2018](https://arxiv.org/abs/1805.04833)) | [WritingPrompts](https://arxiv.org/abs/1805.04833) | [download (.tar.bz2)](https://dl.fbaipublicfiles.com/fairseq/models/stories_checkpoint.tar.bz2) | [download (.tar.bz2)](https://dl.fbaipublicfiles.com/fairseq/data/stories_test.tar.bz2)

We provide sample stories generated by the [convolutional seq2seq model](https://dl.fbaipublicfiles.com/fairseq/data/seq2seq_stories.txt) and [fusion model](https://dl.fbaipublicfiles.com/fairseq/data/fusion_stories.txt) from [Fan et al., 2018](https://arxiv.org/abs/1805.04833).

## Dataset

The dataset can be downloaded like this:

```
cd examples/stories
curl https://dl.fbaipublicfiles.com/fairseq/data/writingPrompts.tar.gz | tar xvzf -
```

and contains a train, test, and valid split. The dataset is described here: https://arxiv.org/abs/1805.04833. We model only the first 1000 words of each story, including one newLine token.


## Example usage

```
# Preprocess the dataset:
# Note that the dataset release is the full data, but the paper models the first 1000 words of each story
# Here is some example code that can trim the dataset to the first 1000 words of each story
$ python
$ data = ["train", "test", "valid"]
$ for name in data:
$   with open(name + ".wp_target") as f:
$     stories = f.readlines()
$   stories = [" ".join(i.split()[0:1000]) for i in stories]
$   with open(name + ".wp_target", "w") as o:
$     for line in stories:
$       o.write(line.strip() + "\n")

# Binarize the dataset:
$ export TEXT=examples/stories/writingPrompts
$ fairseq-preprocess --source-lang wp_source --target-lang wp_target \
  --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \
  --destdir data-bin/writingPrompts --padding-factor 1 --thresholdtgt 10 --thresholdsrc 10

# Train the model:
$ fairseq-train data-bin/writingPrompts -a fconv_self_att_wp --lr 0.25 --clip-norm 0.1 --max-tokens 1500 --lr-scheduler reduce_lr_on_plateau --decoder-attention True --encoder-attention False --criterion label_smoothed_cross_entropy --weight-decay .0000001 --label-smoothing 0 --source-lang wp_source --target-lang wp_target --gated-attention True --self-attention True --project-input True --pretrained False

# Train a fusion model:
# add the arguments: --pretrained True --pretrained-checkpoint path/to/checkpoint

# Generate:
# Note: to load the pretrained model at generation time, you need to pass in a model-override argument to communicate to the fusion model at generation time where you have placed the pretrained checkpoint. By default, it will load the exact path of the fusion model's pretrained model from training time. You should use model-override if you have moved the pretrained model (or are using our provided models). If you are generating from a non-fusion model, the model-override argument is not necessary.

$ fairseq-generate data-bin/writingPrompts --path /path/to/trained/model/checkpoint_best.pt --batch-size 32 --beam 1 --sampling --sampling-topk 10 --sampling-temperature 0.8 --nbest 1 --model-overrides "{'pretrained_checkpoint':'/path/to/pretrained/model/checkpoint'}"
```
# Running the prompt ranking task

The two files for running the prompt ranking task are `make_prompt_ranking.py` and `prompt_ranking.py`. `make_prompt_ranking.py` creates a plaintext file of 1000 randomly sampled gold (prompt, story) pairs from the test set and for each one, randomly samples 9 'fake prompts', and writes the fake (prompt, story) pair to file. 

After binarizing the output files of `make_prompt_ranking.py`, `prompt_ranking.py` takes the binarized data and model checkpoints, and runs the prompt ranking task, where the score is determined as a 10-way classification of whether the gold prompt-story pair is scored higher than all the fake prompt-gold story pairs. `prompt_ranking.py` is adapted from `eval_lm.py`, with very few differences.

**make_prompt_ranking.py**

This simple script takes input .wp_source and .wp_target files and outputs a .wp_source and .wp_target plaintext file for prompt ranking. The script samples 1000 gold prompt-story pairs and for each one, writes the gold prompt and 9 randomly sampled prompts to the .wp_source file and writes the gold story to the .wp_target file 10 times. 

To run, execute the command:

```
$ python make_prompt_ranking.py --datapath /path/to/wpsource_and_target_files
```

Then, use the fairseq  `preprocess.py` script to generate the binarized data format for the output .wp_source and .wp_target files that are created from the previous step, ensuring that you point to the source and target dictionaries generated on the full writingPrompts dataset (so that the binarized data is compatible with the pretrained model). For example, run the command:

```
$ python preprocess.py  --source-lang wp_source --target-lang wp_target --testpref /path/to/text_data --destdir data-bin/writingPromptsPromptRanking --srcdict /path/to/writingPrompts/dict.wp_source.txt --tgtdict dict.wp_target.txt
```

**prompt_ranking.py**

This script takes binarized data, the paths for the fusion checkpoint and pretrained checkpoint, and runs the prompt ranking task. This script is largely based off of the eval_lm.py script, and uses the scorer to score the prompt-story pairs. After all pairs are scored, the script computes the number of gold prompt-story pairs for which they were scored with a higher log probability than the fake prompt-story pairs. 

Note: It's important to run with the flag `--max-sentences` set to 1 so that the prompt-story pairs are processed in the order they were originally written to the text file.

To run prompt_ranking, execute the command:

```
$ python prompt_ranking.py data-bin/writingPromptsPromptRanking/ --path path/to/fusion_checkpoint.pt  --model-overrides “{‘pretrained_checkpoint’: ‘path/to/pretrained_checkpoint.pt’}” --task translation --max-sentences 1
```

## Citation
```bibtex
@inproceedings{fan2018hierarchical,
  title = {Hierarchical Neural Story Generation},
  author = {Fan, Angela and Lewis, Mike and Dauphin, Yann},
  booktitle = {Conference of the Association for Computational Linguistics (ACL)},
  year = 2018,
}
```
